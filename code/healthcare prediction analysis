!pip install pandas numpy matplotlib seaborn scikit-learn joblib

plt.show()

# Correlation Heatmap
plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# ===================================
# 3. Preprocessing
# ===================================

# Remove extra spaces in column names
df.columns = df.columns.str.strip()

# Encode Gender (if text like Male/Female)
if df['Gender'].dtype == 'object':
    le = LabelEncoder()
    df['Gender'] = le.fit_transform(df['Gender'])

# Features and Target
# Check class distribution
print(y.value_counts())

# If imbalance exists
from sklearn.utils import resample

df_majority = df[df.Heart_Risk == 0]
df_minority = df[df.Heart_Risk == 1]
# ===================================
# 1. Import Libraries
# ===================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# ===================================
# 2. Load Dataset
# ===================================
df = pd.read_csv("/content/heart_disease_risk_dataset_earlymed.csv")   # change filename

print("Dataset Shape:", df.shape)
print(df.head())

# ===================================
#  EDA Section
# ===================================

print(df.info())
print(df.describe())

# Check class distribution
sns.countplot(x=df["Heart_Risk"])
plt.title("Heart Risk Distribution")
df_minority_upsampled = resample(df_minority,
                                 replace=True,
                                 n_samples=len(df_majority),
                                 random_state=42)

df_balanced = pd.concat([df_majority, df_minority_upsampled])

X = df_balanced.drop("Heart_Risk", axis=1)
y = df_balanced["Heart_Risk"]


# Scale Age (only numerical continuous column)
scaler = StandardScaler()
X['Age'] = scaler.fit_transform(X[['Age']])

# ===================================
# 4. Train-Test Split
# ===================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 5, 10]
}

grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)

best_model = grid.best_estimator_
print("Best Parameters:", grid.best_params_)


# ===================================
# 5. Train Multiple Models
# ===================================

models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(n_estimators=100)
}

results = {}

for name, model in models.items():


    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy

    print("\n==============================")
    print("Model:", name)
    print("Accuracy:", accuracy)
    print(classification_report(y_test, y_pred))

# ===================================
# 6. Select Best Model
# ===================================
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]

print("\nBest Model:", best_model_name)

# ===================================
# 7. Confusion Matrix
# ===================================
y_pred = best_model.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d")
plt.title("Confusion Matrix")
plt.show()

# ===================================
# 8. ROC Curve
# ===================================
y_prob = best_model.predict_proba(X_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.plot(fpr, tpr)
plt.plot([0,1],[0,1],'--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.show()

print("ROC-AUC Score:", roc_auc_score(y_test, y_prob))
# Risk Level Classification
prob = best_model.predict_proba(X_test)[:,1]

def risk_level(p):
    if p < 0.3:
        return "Low Risk"
    elif p < 0.7:
        return "Medium Risk"
    else:
        return "High Risk"

risk_labels = [risk_level(p) for p in prob]

print("Sample Risk Levels:", risk_labels[:10])


# ===================================
# 9. Cross Validation
# ===================================
cv_scores = cross_val_score(best_model, X, y, cv=5)
print("Cross Validation Accuracy:", np.mean(cv_scores))

# ===================================
# 10. Feature Importance (If RF)
# ===================================
if best_model_name == "Random Forest":
    importances = best_model.feature_importances_
    feature_names = X.columns

    feature_df = pd.DataFrame({
        "Feature": feature_names,
        "Importance": importances
    }).sort_values(by="Importance", ascending=False)

    plt.figure(figsize=(8,5))
    sns.barplot(x="Importance", y="Feature", data=feature_df)
    plt.title("Feature Importance")
    plt.show()

# ===================================
# 11. Save Model
# ===================================
joblib.dump(best_model, "heart_risk_model.pkl")
joblib.dump(scaler, "scaler.pkl")

print("Model Saved Successfully!")
